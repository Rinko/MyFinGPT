{"cells":[{"cell_type":"code","source":["!/opt/bin/nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZs62ycwodky","executionInfo":{"status":"ok","timestamp":1702448794764,"user_tz":-480,"elapsed":483,"user":{"displayName":"John Smith","userId":"00802671440894225228"}},"outputId":"075e303d-ff68-4533-855e-d4236535b81e"},"id":"vZs62ycwodky","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Dec 13 06:26:34 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   47C    P0    28W /  70W |  15039MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["!apt install psmisc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rd0f4k3VzzRz","executionInfo":{"status":"ok","timestamp":1702448799423,"user_tz":-480,"elapsed":2917,"user":{"displayName":"John Smith","userId":"00802671440894225228"}},"outputId":"52fd4176-26f8-4788-8d6a-7dcb22e4a64a"},"id":"rd0f4k3VzzRz","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","psmisc is already the newest version (23.4-2build3).\n","0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n"]}]},{"cell_type":"code","source":["!sudo fuser -v /dev/nvidia*"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lw6eF759z4YM","executionInfo":{"status":"ok","timestamp":1702448806913,"user_tz":-480,"elapsed":485,"user":{"displayName":"John Smith","userId":"00802671440894225228"}},"outputId":"e53af8b1-a0c8-4928-bf26-43b76c354ddd"},"id":"Lw6eF759z4YM","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["                     USER        PID ACCESS COMMAND\n","/dev/nvidia0:        root       6474 F...m python3\n","/dev/nvidiactl:      root       6474 F...m python3\n","/dev/nvidia-uvm:     root       6474 F...m python3\n"]}]},{"cell_type":"code","source":["!pmap -d 723"],"metadata":{"id":"iyzzXm8-0Z-0"},"id":"iyzzXm8-0Z-0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!kill -9 723"],"metadata":{"id":"9D2PP51z0naI"},"id":"9D2PP51z0naI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rx6KZPVUg_Vz","executionInfo":{"status":"ok","timestamp":1702447100475,"user_tz":-480,"elapsed":88725,"user":{"displayName":"John Smith","userId":"00802671440894225228"}},"outputId":"bd090216-f3e4-4acb-a7e4-fe66c802d9b2"},"id":"Rx6KZPVUg_Vz","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install --upgrade pip\n","!pip install -r /content/drive/MyDrive/fin/FinGPT/fingpt/FinGPT_Forecaster/requirements.txt\n","!pip install utils"],"metadata":{"id":"CfJk0ODxhknO"},"id":"CfJk0ODxhknO","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":7,"id":"16172382-ba5f-4051-bbe7-93b32d958e09","metadata":{"id":"16172382-ba5f-4051-bbe7-93b32d958e09","executionInfo":{"status":"ok","timestamp":1702448469550,"user_tz":-480,"elapsed":11287,"user":{"displayName":"John Smith","userId":"00802671440894225228"}}},"outputs":[],"source":["import torch\n","from datasets import load_from_disk\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel, PeftConfig\n","from utils import *\n","from huggingface_hub import login"]},{"cell_type":"code","source":["login(token=\"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\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UI-d-Dpaq_sx","executionInfo":{"status":"ok","timestamp":1702448485348,"user_tz":-480,"elapsed":499,"user":{"displayName":"John Smith","userId":"00802671440894225228"}},"outputId":"52a6b688-8fad-4739-aca2-e91573fb663d"},"id":"UI-d-Dpaq_sx","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","execution_count":9,"id":"f9ce84bf-0759-46c2-81cb-c16a1e3f6cfd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":77,"referenced_widgets":["d1469b7daea94523bf98d0f601ba585a","f3e82501ec14463da0ac95213bb81d3c","a723a87217374daf9b1a8223ebed828b","e08bafc95db04ddcac462e658d068754","8ceec0cc4efc4102826131606824e584","fc31a22ef08e4a05a4bca229110fcb04","733f5904227a4bb19609dc36f23219a8","29b0d6c1d451402ea432c71368749695","226bd01e31374a44b15f0284ae8b7e4f","7558a6377b6a4c37bb6f200d1848e7fd","84afdc73bd254c5eba6355bb66c44a66"]},"id":"f9ce84bf-0759-46c2-81cb-c16a1e3f6cfd","executionInfo":{"status":"ok","timestamp":1702448552155,"user_tz":-480,"elapsed":64768,"user":{"displayName":"John Smith","userId":"00802671440894225228"}},"outputId":"9c3f8afd-21b5-460c-e3db-5fff82e152e6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1469b7daea94523bf98d0f601ba585a"}},"metadata":{}}],"source":["base_model = AutoModelForCausalLM.from_pretrained(\n","    'meta-llama/Llama-2-7b-chat-hf',\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n","    offload_folder=\"offload\",\n","    torch_dtype=torch.float16,\n",")\n","base_model.model_parellal = True"]},{"cell_type":"code","execution_count":10,"id":"213a792c-fe67-49be-a340-98e18bbc50eb","metadata":{"id":"213a792c-fe67-49be-a340-98e18bbc50eb","executionInfo":{"status":"ok","timestamp":1702448670591,"user_tz":-480,"elapsed":109935,"user":{"displayName":"John Smith","userId":"00802671440894225228"}}},"outputs":[],"source":["config = PeftConfig.from_pretrained(\"FinGPT/fingpt-forecaster_dow30_llama2-7b_lora\")\n","model = PeftModel.from_pretrained(\n","    base_model,\n","    \"FinGPT/fingpt-forecaster_dow30_llama2-7b_lora\",\n","    torch_dtype=torch.float16,\n","    offload_folder=\"offload\"\n",")\n","model = model.eval()"]},{"cell_type":"code","execution_count":11,"id":"8cf0220b-9173-4d1c-a4a4-119bcda71cfe","metadata":{"id":"8cf0220b-9173-4d1c-a4a4-119bcda71cfe","executionInfo":{"status":"ok","timestamp":1702448677968,"user_tz":-480,"elapsed":498,"user":{"displayName":"John Smith","userId":"00802671440894225228"}}},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n","tokenizer.padding_side = \"right\"\n","tokenizer.pad_token_id = tokenizer.eos_token_id"]},{"cell_type":"code","execution_count":12,"id":"d915b597-258c-4e8b-9f8f-7fc0e2d8bd75","metadata":{"id":"d915b597-258c-4e8b-9f8f-7fc0e2d8bd75","executionInfo":{"status":"ok","timestamp":1702448686900,"user_tz":-480,"elapsed":494,"user":{"displayName":"John Smith","userId":"00802671440894225228"}}},"outputs":[],"source":["START_DATE = \"2023-5-31\"\n","END_DATE = \"2023-11-30\"\n","\n","DATA_DIR = f\"/content/drive/MyDrive/fin/FinGPT/fingpt/FinGPT_Forecaster/data/{START_DATE}_{END_DATE}\"\n","\n","test_dataset = load_from_disk(f'{DATA_DIR}/fingpt-forecaster-dow30v3-{START_DATE}_{END_DATE}-llama')['test']"]},{"cell_type":"code","execution_count":13,"id":"fe9b99e6-f4be-4dcc-820a-ccb95d819904","metadata":{"id":"fe9b99e6-f4be-4dcc-820a-ccb95d819904","executionInfo":{"status":"ok","timestamp":1702448692786,"user_tz":-480,"elapsed":470,"user":{"displayName":"John Smith","userId":"00802671440894225228"}}},"outputs":[],"source":["def test_demo(model, tokenizer, prompt):\n","\n","    inputs = tokenizer(\n","        prompt, return_tensors='pt',\n","        padding=False, max_length=4096\n","    )\n","    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n","\n","    res = model.generate(\n","        **inputs, max_length=4096, do_sample=True,\n","        eos_token_id=tokenizer.eos_token_id,\n","        use_cache=True\n","    )\n","    output = tokenizer.decode(res[0], skip_special_tokens=True)\n","    return output\n","    # return res"]},{"cell_type":"code","execution_count":14,"id":"8c0c554b-ec30-47e3-af01-5cede7cdbbe4","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":569},"id":"8c0c554b-ec30-47e3-af01-5cede7cdbbe4","executionInfo":{"status":"error","timestamp":1702448711379,"user_tz":-480,"elapsed":4793,"user":{"displayName":"John Smith","userId":"00802671440894225228"}},"outputId":"5ff521d1-98b6-48e6-87e5-094cb2b7afa8"},"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-c9c43d632afb>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prompt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'.*\\[/INST\\]\\s*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOTALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-9aeedb55e0a0>\u001b[0m in \u001b[0;36mtest_demo\u001b[0;34m(model, tokenizer, prompt)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     res = model.generate(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prepare_inputs_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1643\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2724\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2725\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    810\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    695\u001b[0m                 )\n\u001b[1;32m    696\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    698\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# upcast attention to fp32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 14.75 GiB total capacity; 13.76 GiB already allocated; 62.81 MiB free; 13.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["answers, gts = [], []\n","\n","for i in range(len(test_dataset)):\n","    prompt = test_dataset[i]['prompt']\n","    output = test_demo(model, tokenizer, prompt)\n","    answer = re.sub(r'.*\\[/INST\\]\\s*', '', output, flags=re.DOTALL)\n","    gt = test_dataset[i]['answer']\n","    print('\\n------- Prompt ------\\n')\n","    print(prompt)\n","    print('\\n------- LLaMA2 Finetuned ------\\n')\n","    print(answer)\n","    print('\\n------- GPT4 Groundtruth ------\\n')\n","    print(gt)\n","    print('\\n===============\\n')\n","    answers.append(answer)\n","    gts.append(gt)"]},{"cell_type":"code","execution_count":null,"id":"931acd45-6f0b-436e-b40e-4a287c331148","metadata":{"id":"931acd45-6f0b-436e-b40e-4a287c331148","outputId":"c71c6c59-8eb5-4f48-e61c-36d2ea43724b"},"outputs":[{"data":{"text/plain":["\"[Positive Developments]:\\n1. The declaration of a regular quarterly dividend, amounting to $0.60 per common share, indicates a steady cash return to shareholders. This can boost investor confidence, leading to increased demand for AXP shares.\\n2. A company executive's participation in major industry conferences offers the chance to elaborate on American Express's growth strategy and financial performance. Such events could provide positive exposure for the company, potentially driving stock price gains.\\n3. The article highlighting American Express's double-digit revenue growth and aggressive share buyback strategy illustrates strong company performance, a factor that can lead to increased investor interest.\\n\\n[Potential Concerns]:\\n1. The interest rate hike by the Federal Reserve could put pressure on financial institutions, affecting their loans' profitability and potentially leading to a decline in stock prices.\\n2. According to one report, concerns exist that credit card profitability may have peaked following a strong 2022. If this is the case, future earnings might not meet analyst expectations, leading to potential stock price declines.\\n3. Widespread market turbulence, as highlighted in the news articles, can negatively impact AXP's stock price due to increased risk aversion from investors.\\n\\n[Prediction & Analysis]:\\nPrediction: Up by 2-3%\\nAnalysis: Considering the aforementioned factors, my prediction for AXP's stock price movement for the upcoming week (2023-05-21 to 2023-05-28) is a potential increase of 2-3%. Despite the concerns, the positive developments present a more compelling case for the company's near-term performance. The declaration of a quarterly dividend may incentivize new investors, supporting the stock price. Additionally, the company's aggressive share buyback strategy, coupled with its double-digit revenue growth, underscore its strong financial position. This, along with the opportunity for company executives to share key insights during industry conferences, could build further investor confidence, aiding the upward movement of AXP's stock price. However, it's critical to monitor the potential concerns, as any unexpected negative developments could sway the trajectory of the stock.\""]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["gts[0]"]},{"cell_type":"code","execution_count":null,"id":"85aeac59-c373-470e-9619-403515f707a6","metadata":{"id":"85aeac59-c373-470e-9619-403515f707a6","outputId":"6cb15c35-008d-45fd-bf39-ecc42f2b43a6"},"outputs":[{"data":{"text/plain":["\"[Positive Developments]:\\n1. The company's Vice Chairman, Douglas E. Buckminster, will participate in the SVB MoffettNathanson Technology, Media, and Telecom Conference, which could potentially increase investor confidence and interest in the company.\\n2. American Express declared a regular quarterly dividend, which may attract dividend-focused investors.\\n3. The company's stock price increased from 146.82 to 151.80 in the week from 2023-05-14 to 2023-05-21, indicating potential upward trends.\\n\\n[Potential Concerns]:\\n1. The company's stock price has been declining since the beginning of May, which may indicate a potential downward trend.\\n2. There have been concerns about the company's profitability after a strong 2022, which could indicate a potential peak in profitability.\\n3. The company's stock has been mentioned in articles discussing the potential risks of fintech companies and the regional banking crisis, which could impact the company's profitability and stock price.\\n\\n[Prediction & Analysis]:\\nPrediction: Up by 1-2%\\nAnalysis: While the company has faced some concerns about profitability and the overall financial market, there are also positive developments such as the declaration of a regular quarterly dividend and the upcoming participation of the company's Vice Chairman in a key conference. These developments could potentially attract investors and drive up the stock price. Additionally, the stock price has shown some positive movement in the previous week, which could indicate potential for further growth. However, the overall market conditions and the company's profitability remain a concern, and the stock price could potentially be impacted by these factors. Therefore, a cautious outlook is recommended, with a prediction of a 1-2% increase in the stock price for the upcoming week.\""]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["answers[0]"]},{"cell_type":"code","execution_count":null,"id":"d297b2e8-dcf0-4b2c-880a-43ecc0055bb1","metadata":{"id":"d297b2e8-dcf0-4b2c-880a-43ecc0055bb1","outputId":"533a36f4-da2e-4be0-9b5c-6f22b4ebb6fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Binary Accuracy: 0.61  |  Mean Square Error: 5.93\n","\n","Rouge Score of Positive Developments: {'rouge1': 0.43196333904653406, 'rouge2': 0.15722817931423375, 'rougeL': 0.264313106777193}\n","\n","Rouge Score of Potential Concerns: {'rouge1': 0.39554620999107204, 'rouge2': 0.12632646861001814, 'rougeL': 0.24735208697254654}\n","\n","Rouge Score of Summary Analysis: {'rouge1': 0.42928213837028545, 'rouge2': 0.1354372841499895, 'rougeL': 0.2320299101133297}\n"]},{"data":{"text/plain":["{'valid_count': 57,\n"," 'bin_acc': 0.6140350877192983,\n"," 'mse': 5.9298245614035086,\n"," 'pros_rouge_scores': {'rouge1': 0.43196333904653406,\n","  'rouge2': 0.15722817931423375,\n","  'rougeL': 0.264313106777193},\n"," 'cons_rouge_scores': {'rouge1': 0.39554620999107204,\n","  'rouge2': 0.12632646861001814,\n","  'rougeL': 0.24735208697254654},\n"," 'anal_rouge_scores': {'rouge1': 0.42928213837028545,\n","  'rouge2': 0.1354372841499895,\n","  'rougeL': 0.2320299101133297}}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["calc_metrics(answers, gts)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d1469b7daea94523bf98d0f601ba585a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3e82501ec14463da0ac95213bb81d3c","IPY_MODEL_a723a87217374daf9b1a8223ebed828b","IPY_MODEL_e08bafc95db04ddcac462e658d068754"],"layout":"IPY_MODEL_8ceec0cc4efc4102826131606824e584"}},"f3e82501ec14463da0ac95213bb81d3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc31a22ef08e4a05a4bca229110fcb04","placeholder":"​","style":"IPY_MODEL_733f5904227a4bb19609dc36f23219a8","value":"Loading checkpoint shards: 100%"}},"a723a87217374daf9b1a8223ebed828b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_29b0d6c1d451402ea432c71368749695","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_226bd01e31374a44b15f0284ae8b7e4f","value":2}},"e08bafc95db04ddcac462e658d068754":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7558a6377b6a4c37bb6f200d1848e7fd","placeholder":"​","style":"IPY_MODEL_84afdc73bd254c5eba6355bb66c44a66","value":" 2/2 [00:57&lt;00:00, 26.10s/it]"}},"8ceec0cc4efc4102826131606824e584":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc31a22ef08e4a05a4bca229110fcb04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"733f5904227a4bb19609dc36f23219a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29b0d6c1d451402ea432c71368749695":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"226bd01e31374a44b15f0284ae8b7e4f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7558a6377b6a4c37bb6f200d1848e7fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84afdc73bd254c5eba6355bb66c44a66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}